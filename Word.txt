PySpark is the Python API for Spark.

Public classes:

SparkContext:
Main entry point for Spark functionality.

RDD:
A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.

Broadcast:
A broadcast variable that gets reused across tasks.

Accumulator:
An “add-only” shared variable that tasks can only add values to.

SparkConf:
For configuring Spark.

SparkFiles:
Access files shipped with jobs.

StorageLevel:
Finer-grained cache persistence levels.

TaskContext:
Information about the current running task, available on the workers and experimental.

RDDBarrier:
Wraps an RDD under a barrier stage for barrier execution.

BarrierTaskContext:
A TaskContext that provides extra info and tooling for barrier execution.

BarrierTaskInfo:
Information about a barrier task.

class pyspark.SparkConf(loadDefaults=True, _jvm=None, _jconf=None)[source]
Configuration for a Spark application. Used to set various Spark parameters as key-value pairs.

Most of the time, you would create a SparkConf object with SparkConf(), which will load values from spark.* Java system properties as well. In this case, any parameters you set directly on the SparkConf object take priority over system properties.

For unit tests, you can also call SparkConf(false) to skip loading external settings and get the same configuration no matter what the system properties are.

All setter methods in this class support chaining. For example, you can write conf.setMaster("local").setAppName("My app").